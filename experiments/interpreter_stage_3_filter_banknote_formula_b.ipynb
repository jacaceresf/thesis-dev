{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Execute imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from genetic_selection import GeneticSelectionCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data_glass = pd.read_csv('datasets/banknote-authentication.csv', delimiter=',')\n",
    "X = pd.read_csv('datasets/banknote_formula_b.csv', delimiter=',')\n",
    "y = data_glass['Class']\n",
    "features = list(X.columns.values)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('selector', SelectKBest(mutual_info_classif, k='all')),\n",
    "                 ('classifier', KNeighborsClassifier())])\n",
    "\n",
    "search_space = [{'selector__k': ['all']},\n",
    "                {'classifier': [KNeighborsClassifier()]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.72 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('selector',\n",
       "                                        SelectKBest(k='all',\n",
       "                                                    score_func=<function mutual_info_classif at 0x000002256416BB80>)),\n",
       "                                       ('classifier', KNeighborsClassifier())]),\n",
       "             param_grid=[{'selector__k': ['all']},\n",
       "                         {'classifier': [KNeighborsClassifier()]}])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "knn_info = GridSearchCV(pipe, search_space, cv=None, verbose=0)\n",
    "knn_info.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9875965302020523\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(knn_info, X, y, cv=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">acc=1.000, est=0.913, cfg={'selector__k': 10}\n",
      ">acc=0.996, est=0.920, cfg={'selector__k': 10}\n",
      ">acc=1.000, est=0.913, cfg={'selector__k': 10}\n",
      ">acc=1.000, est=0.914, cfg={'selector__k': 10}\n",
      ">acc=1.000, est=0.909, cfg={'selector__k': 10}\n",
      "Accuracy: 0.999 (0.001)\n",
      "Wall time: 7.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# enumerate splits\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in kf.split(X):\n",
    "    # split data\n",
    "    X_train, X_test = X.iloc[train_ix].loc[:, features], X.iloc[test_ix][features]\n",
    "    y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "    # define the model\n",
    "    pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('selector', SelectKBest(mutual_info_classif, k=10)),\n",
    "                 ('classifier', KNeighborsClassifier())])\n",
    "    # define search space\n",
    "    search_space = [{'selector__k': [10]}, {'classifier': [KNeighborsClassifier()]}]\n",
    "    # define search\n",
    "    _info = GridSearchCV(pipe, search_space, scoring='accuracy', cv=None, refit=True)\n",
    "    # execute search\n",
    "    result = _info.fit(X_train, y_train)\n",
    "    \n",
    "    selected = result.best_estimator_.named_steps['selector'].get_support(indices=True)\n",
    "    selected_features = X.iloc[:,selected]\n",
    "    x_t = X_test[selected_features.columns.values]\n",
    "    \n",
    "    clf = KNeighborsClassifier()\n",
    "    clf.fit(X_train[selected_features.columns.values], y_train)\n",
    "    # evaluate the model\n",
    "    acc = accuracy_score(y_test, clf.predict(x_t))\n",
    "    # store the result\n",
    "    outer_results.append(acc)\n",
    "    # report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_params_))\n",
    "# summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(outer_results), np.std(outer_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "pipe = Pipeline([('selector', VarianceThreshold()),\n",
    "                 ('classifier', KNeighborsClassifier())])\n",
    "\n",
    "search_space = [{'classifier': [KNeighborsClassifier()]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 183 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('selector', VarianceThreshold()),\n",
       "                                       ('classifier', KNeighborsClassifier())]),\n",
       "             param_grid=[{'classifier': [KNeighborsClassifier()]}],\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "knn_variance = GridSearchCV(pipe, search_space, cv=None, verbose=0, scoring= 'accuracy')\n",
    "knn_variance.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(knn_variance, X, y, cv=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">acc=1.000, est=1.000, cfg=Pipeline(steps=[('selector', VarianceThreshold()),\n",
      "                ('classifier', KNeighborsClassifier())])\n",
      ">acc=0.996, est=1.000, cfg=Pipeline(steps=[('selector', VarianceThreshold()),\n",
      "                ('classifier', KNeighborsClassifier())])\n",
      ">acc=1.000, est=1.000, cfg=Pipeline(steps=[('selector', VarianceThreshold()),\n",
      "                ('classifier', KNeighborsClassifier())])\n",
      ">acc=1.000, est=1.000, cfg=Pipeline(steps=[('selector', VarianceThreshold()),\n",
      "                ('classifier', KNeighborsClassifier())])\n",
      ">acc=1.000, est=1.000, cfg=Pipeline(steps=[('selector', VarianceThreshold()),\n",
      "                ('classifier', KNeighborsClassifier())])\n",
      "Accuracy: 0.999 (0.001)\n",
      "Wall time: 683 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features = list(X.columns.values)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# enumerate splits\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in kf.split(X):\n",
    "    # split data\n",
    "    X_train, X_test = X.iloc[train_ix].loc[:, features], X.iloc[test_ix][features]\n",
    "    y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "    # define the model\n",
    "    pipe = Pipeline([('selector', VarianceThreshold()),\n",
    "                 ('classifier', KNeighborsClassifier())])\n",
    "\n",
    "    search_space = [{'classifier': [KNeighborsClassifier()]}]\n",
    "    # define search\n",
    "    _info = GridSearchCV(pipe, search_space, scoring='accuracy', cv=None, refit=True)\n",
    "    # execute search\n",
    "    result = _info.fit(X_train, y_train)\n",
    "    \n",
    "    select_indexes = np.arange(len(X_train.columns)).reshape(1, -1)\n",
    "    select_indexes = result.best_estimator_.named_steps['selector'].transform(select_indexes)\n",
    "    selected_features = X.iloc[:,select_indexes.reshape(-1)]\n",
    "    x_t = X_test[selected_features.columns.values]\n",
    "    \n",
    "    clf = KNeighborsClassifier()\n",
    "    clf.fit(X_train[selected_features.columns.values], y_train)\n",
    "    # evaluate the model\n",
    "    acc = accuracy_score(y_test, clf.predict(x_t))\n",
    "    # store the result\n",
    "    outer_results.append(acc)\n",
    "    # report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_estimator_))\n",
    "# summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(outer_results), np.std(outer_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "pipe = Pipeline([('scaler', MinMaxScaler()),\n",
    "                 ('selector', SelectKBest(chi2, k='all')),\n",
    "                 ('classifier', KNeighborsClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = [{'classifier': [KNeighborsClassifier()]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 187 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('scaler', MinMaxScaler()),\n",
       "                                       ('selector',\n",
       "                                        SelectKBest(k='all',\n",
       "                                                    score_func=<function chi2 at 0x0000022562F90040>)),\n",
       "                                       ('classifier', KNeighborsClassifier())]),\n",
       "             param_grid=[{'classifier': [KNeighborsClassifier()]}])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "knn_chi = GridSearchCV(pipe, search_space, cv=None, verbose=0)\n",
    "knn_chi.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9606315455410982\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(knn_chi, X, y, cv=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">acc=1.000, est=0.825, cfg=Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('selector',\n",
      "                 SelectKBest(score_func=<function chi2 at 0x000001AA93E00DC0>)),\n",
      "                ('classifier', KNeighborsClassifier())])\n",
      ">acc=0.996, est=0.836, cfg=Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('selector',\n",
      "                 SelectKBest(score_func=<function chi2 at 0x000001AA93E00DC0>)),\n",
      "                ('classifier', KNeighborsClassifier())])\n",
      ">acc=1.000, est=0.820, cfg=Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('selector',\n",
      "                 SelectKBest(score_func=<function chi2 at 0x000001AA93E00DC0>)),\n",
      "                ('classifier', KNeighborsClassifier())])\n",
      ">acc=1.000, est=0.832, cfg=Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('selector',\n",
      "                 SelectKBest(score_func=<function chi2 at 0x000001AA93E00DC0>)),\n",
      "                ('classifier', KNeighborsClassifier())])\n",
      ">acc=1.000, est=0.823, cfg=Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('selector',\n",
      "                 SelectKBest(score_func=<function chi2 at 0x000001AA93E00DC0>)),\n",
      "                ('classifier', KNeighborsClassifier())])\n",
      "Accuracy: 0.999 (0.001)\n",
      "Wall time: 867 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features = list(X.columns.values)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# enumerate splits\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in kf.split(X):\n",
    "    # split data\n",
    "    X_train, X_test = X.iloc[train_ix][features], X.iloc[test_ix][features]\n",
    "    y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "    # define the model\n",
    "    pipe = Pipeline([('scaler', MinMaxScaler()),\n",
    "                 ('selector', SelectKBest(chi2, k=10)),\n",
    "                 ('classifier', KNeighborsClassifier())])\n",
    "    # define search space\n",
    "    search_space = [{'classifier': [KNeighborsClassifier()]}]\n",
    "    # define search\n",
    "    _info = GridSearchCV(pipe, search_space, scoring='accuracy', cv=None, refit=True)\n",
    "    # execute search\n",
    "    result = _info.fit(X_train, y_train)\n",
    "    \n",
    "    selected = result.best_estimator_.named_steps['selector'].get_support(indices=True)\n",
    "    selected_features = X.iloc[:,selected]\n",
    "    x_t = X_test[selected_features.columns.values]\n",
    "    \n",
    "    clf = KNeighborsClassifier()\n",
    "    clf.fit(X_train[selected_features.columns.values], y_train)\n",
    "    # evaluate the model\n",
    "    acc = accuracy_score(y_test, clf.predict(x_t))\n",
    "    # store the result\n",
    "    outer_results.append(acc)\n",
    "    # report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_estimator_))\n",
    "# summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(outer_results), np.std(outer_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('selector', SelectKBest(mutual_info_classif, k='all')),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "search_space = [{'selector__k': ['all']},\n",
    "                {'classifier': [LogisticRegression()]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.16 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('selector',\n",
       "                                        SelectKBest(k='all',\n",
       "                                                    score_func=<function mutual_info_classif at 0x000002256416BB80>)),\n",
       "                                       ('classifier', LogisticRegression())]),\n",
       "             param_grid=[{'selector__k': ['all']},\n",
       "                         {'classifier': [LogisticRegression()]}])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "logistic_info = GridSearchCV(pipe, search_space, cv=None, verbose=0)\n",
    "logistic_info.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.982502909129377\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(logistic_info, X, y, cv=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">acc=0.993, est=0.984, cfg={'selector__k': 10}\n",
      ">acc=0.985, est=0.980, cfg={'selector__k': 10}\n",
      ">acc=0.993, est=0.977, cfg={'selector__k': 10}\n",
      ">acc=0.985, est=0.982, cfg={'selector__k': 10}\n",
      ">acc=0.993, est=0.982, cfg={'selector__k': 10}\n",
      "Accuracy: 0.990 (0.004)\n",
      "Wall time: 8.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# enumerate splits\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in kf.split(X):\n",
    "    # split data\n",
    "    X_train, X_test = X.iloc[train_ix].loc[:, features], X.iloc[test_ix][features]\n",
    "    y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "    # define the model\n",
    "    pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('selector', SelectKBest(mutual_info_classif, k=10)),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "    # define search space\n",
    "    search_space = [{'selector__k': [10]}, {'classifier': [LogisticRegression()]}]\n",
    "    # define search\n",
    "    _info = GridSearchCV(pipe, search_space, scoring='accuracy', cv=None, refit=True)\n",
    "    # execute search\n",
    "    result = _info.fit(X_train, y_train)\n",
    "    \n",
    "    selected = result.best_estimator_.named_steps['selector'].get_support(indices=True)\n",
    "    selected_features = X.iloc[:,selected]\n",
    "    x_t = X_test[selected_features.columns.values]\n",
    "    \n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train[selected_features.columns.values], y_train)\n",
    "    # evaluate the model\n",
    "    acc = accuracy_score(y_test, clf.predict(x_t))\n",
    "    # store the result\n",
    "    outer_results.append(acc)\n",
    "    # report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_params_))\n",
    "# summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(outer_results), np.std(outer_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "pipe = Pipeline([('selector', VarianceThreshold()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "search_space = [{'classifier': [LogisticRegression()]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 262 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('selector', VarianceThreshold()),\n",
       "                                       ('classifier', LogisticRegression())]),\n",
       "             param_grid=[{'classifier': [LogisticRegression()]}],\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "logistic_variance = GridSearchCV(pipe, search_space, cv=None, verbose=0, scoring= 'accuracy')\n",
    "logistic_variance.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.994895800275045\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(logistic_variance, X, y, cv=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">acc=0.993, est=0.988, cfg=Pipeline(steps=[('selector', VarianceThreshold()),\n",
      "                ('classifier', LogisticRegression())])\n",
      ">acc=0.985, est=0.990, cfg=Pipeline(steps=[('selector', VarianceThreshold()),\n",
      "                ('classifier', LogisticRegression())])\n",
      ">acc=0.993, est=0.989, cfg=Pipeline(steps=[('selector', VarianceThreshold()),\n",
      "                ('classifier', LogisticRegression())])\n",
      ">acc=0.985, est=0.993, cfg=Pipeline(steps=[('selector', VarianceThreshold()),\n",
      "                ('classifier', LogisticRegression())])\n",
      ">acc=0.993, est=0.990, cfg=Pipeline(steps=[('selector', VarianceThreshold()),\n",
      "                ('classifier', LogisticRegression())])\n",
      "Accuracy: 0.990 (0.004)\n",
      "Wall time: 1.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features = list(X.columns.values)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# enumerate splits\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in kf.split(X):\n",
    "    # split data\n",
    "    X_train, X_test = X.iloc[train_ix].loc[:, features], X.iloc[test_ix][features]\n",
    "    y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "    # define the model\n",
    "    pipe = Pipeline([('selector', VarianceThreshold()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "    search_space = [{'classifier': [LogisticRegression()]}]\n",
    "    # define search\n",
    "    _info = GridSearchCV(pipe, search_space, scoring='accuracy', cv=None, refit=True)\n",
    "    # execute search\n",
    "    result = _info.fit(X_train, y_train)\n",
    "    \n",
    "    select_indexes = np.arange(len(X_train.columns)).reshape(1, -1)\n",
    "    select_indexes = result.best_estimator_.named_steps['selector'].transform(select_indexes)\n",
    "    selected_features = X.iloc[:,select_indexes.reshape(-1)]\n",
    "    x_t = X_test[selected_features.columns.values]\n",
    "    \n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train[selected_features.columns.values], y_train)\n",
    "    # evaluate the model\n",
    "    acc = accuracy_score(y_test, clf.predict(x_t))\n",
    "    # store the result\n",
    "    outer_results.append(acc)\n",
    "    # report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_estimator_))\n",
    "# summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(outer_results), np.std(outer_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "pipe = Pipeline([('scaler', MinMaxScaler()),\n",
    "                 ('selector', SelectKBest(chi2, k='all')),\n",
    "                 ('classifier', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = [{'classifier': [LogisticRegression()]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 194 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('scaler', MinMaxScaler()),\n",
       "                                       ('selector',\n",
       "                                        SelectKBest(k='all',\n",
       "                                                    score_func=<function chi2 at 0x0000022562F90040>)),\n",
       "                                       ('classifier', LogisticRegression())]),\n",
       "             param_grid=[{'classifier': [LogisticRegression()]}])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "logistic_chi = GridSearchCV(pipe, search_space, cv=None, verbose=0)\n",
    "logistic_chi.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9628054585845763\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(logistic_chi, X, y, cv=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">acc=0.993, est=0.969, cfg=Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('selector',\n",
      "                 SelectKBest(score_func=<function chi2 at 0x000001AA93E00DC0>)),\n",
      "                ('classifier', LogisticRegression())])\n",
      ">acc=0.985, est=0.974, cfg=Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('selector',\n",
      "                 SelectKBest(score_func=<function chi2 at 0x000001AA93E00DC0>)),\n",
      "                ('classifier', LogisticRegression())])\n",
      ">acc=0.993, est=0.964, cfg=Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('selector',\n",
      "                 SelectKBest(score_func=<function chi2 at 0x000001AA93E00DC0>)),\n",
      "                ('classifier', LogisticRegression())])\n",
      ">acc=0.985, est=0.966, cfg=Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('selector',\n",
      "                 SelectKBest(score_func=<function chi2 at 0x000001AA93E00DC0>)),\n",
      "                ('classifier', LogisticRegression())])\n",
      ">acc=0.993, est=0.965, cfg=Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('selector',\n",
      "                 SelectKBest(score_func=<function chi2 at 0x000001AA93E00DC0>)),\n",
      "                ('classifier', LogisticRegression())])\n",
      "Accuracy: 0.990 (0.004)\n",
      "Wall time: 747 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features = list(X.columns.values)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# enumerate splits\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in kf.split(X):\n",
    "    # split data\n",
    "    X_train, X_test = X.iloc[train_ix][features], X.iloc[test_ix][features]\n",
    "    y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "    # define the model\n",
    "    pipe = Pipeline([('scaler', MinMaxScaler()),\n",
    "                 ('selector', SelectKBest(chi2, k=10)),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "    # define search space\n",
    "    search_space = [{'classifier': [LogisticRegression()]}]\n",
    "    # define search\n",
    "    _info = GridSearchCV(pipe, search_space, scoring='accuracy', cv=None, refit=True)\n",
    "    # execute search\n",
    "    result = _info.fit(X_train, y_train)\n",
    "    \n",
    "    selected = result.best_estimator_.named_steps['selector'].get_support(indices=True)\n",
    "    selected_features = X.iloc[:,selected]\n",
    "    x_t = X_test[selected_features.columns.values]\n",
    "    \n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train[selected_features.columns.values], y_train)\n",
    "    # evaluate the model\n",
    "    acc = accuracy_score(y_test, clf.predict(x_t))\n",
    "    # store the result\n",
    "    outer_results.append(acc)\n",
    "    # report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_estimator_))\n",
    "# summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(outer_results), np.std(outer_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('selector', SelectKBest(mutual_info_classif, k='all')),\n",
    "                 ('classifier', SVC(kernel='linear'))])\n",
    "\n",
    "search_space = [{'selector__k': ['all']},\n",
    "                {'classifier': [SVC(kernel='linear')]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('selector',\n",
       "                                        SelectKBest(k='all',\n",
       "                                                    score_func=<function mutual_info_classif at 0x000002256416BB80>)),\n",
       "                                       ('classifier', SVC(kernel='linear'))]),\n",
       "             param_grid=[{'selector__k': ['all']},\n",
       "                         {'classifier': [SVC(kernel='linear')]}])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "svm_info = GridSearchCV(pipe, search_space, cv=None, verbose=0)\n",
    "svm_info.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9897968898762297\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(svm_info, X, y, cv=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">acc=0.985, est=0.984, cfg={'selector__k': 9}\n",
      ">acc=0.985, est=0.985, cfg={'selector__k': 9}\n",
      ">acc=0.989, est=0.983, cfg={'selector__k': 9}\n",
      ">acc=0.985, est=0.987, cfg={'selector__k': 9}\n",
      ">acc=0.993, est=0.985, cfg={'selector__k': 9}\n",
      "Accuracy: 0.988 (0.003)\n",
      "Wall time: 7.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# enumerate splits\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in kf.split(X):\n",
    "    # split data\n",
    "    X_train, X_test = X.iloc[train_ix].loc[:, features], X.iloc[test_ix][features]\n",
    "    y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "    # define the model\n",
    "    pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('selector', SelectKBest(mutual_info_classif, k=9)),\n",
    "                 ('classifier', SVC(kernel='linear'))])\n",
    "    # define search space\n",
    "    search_space = [{'selector__k': [9]}, {'classifier': [SVC(kernel='linear')]}]\n",
    "    # define search\n",
    "    _info = GridSearchCV(pipe, search_space, scoring='accuracy', cv=None, refit=True)\n",
    "    # execute search\n",
    "    result = _info.fit(X_train, y_train)\n",
    "    \n",
    "    selected = result.best_estimator_.named_steps['selector'].get_support(indices=True)\n",
    "    selected_features = X.iloc[:,selected]\n",
    "    x_t = X_test[selected_features.columns.values]\n",
    "    \n",
    "    clf = SVC(kernel='linear')\n",
    "    clf.fit(X_train[selected_features.columns.values], y_train)\n",
    "    # evaluate the model\n",
    "    acc = accuracy_score(y_test, clf.predict(x_t))\n",
    "    # store the result\n",
    "    outer_results.append(acc)\n",
    "    # report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_params_))\n",
    "# summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(outer_results), np.std(outer_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "pipe = Pipeline([('selector', VarianceThreshold()),\n",
    "                 ('classifier', SVC(kernel='linear'))])\n",
    "\n",
    "search_space = [{'classifier': [SVC(kernel='linear')]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 130 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('selector', VarianceThreshold()),\n",
       "                                       ('classifier', SVC(kernel='linear'))]),\n",
       "             param_grid=[{'classifier': [SVC(kernel='linear')]}],\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "svm_variance = GridSearchCV(pipe, search_space, cv=None, verbose=0, scoring= 'accuracy')\n",
    "svm_variance.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9963503649635037\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(svm_variance, X, y, cv=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">acc=0.993, est=0.987, cfg=Pipeline(steps=[('selector', VarianceThreshold(threshold=3)),\n",
      "                ('classifier', SVC(kernel='linear'))])\n",
      ">acc=0.985, est=0.988, cfg=Pipeline(steps=[('selector', VarianceThreshold(threshold=3)),\n",
      "                ('classifier', SVC(kernel='linear'))])\n",
      ">acc=0.989, est=0.988, cfg=Pipeline(steps=[('selector', VarianceThreshold(threshold=3)),\n",
      "                ('classifier', SVC(kernel='linear'))])\n",
      ">acc=0.985, est=0.992, cfg=Pipeline(steps=[('selector', VarianceThreshold(threshold=3)),\n",
      "                ('classifier', SVC(kernel='linear'))])\n",
      ">acc=0.993, est=0.991, cfg=Pipeline(steps=[('selector', VarianceThreshold(threshold=3)),\n",
      "                ('classifier', SVC(kernel='linear'))])\n",
      "Accuracy: 0.989 (0.003)\n",
      "Wall time: 560 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features = list(X.columns.values)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# enumerate splits\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in kf.split(X):\n",
    "    # split data\n",
    "    X_train, X_test = X.iloc[train_ix].loc[:, features], X.iloc[test_ix][features]\n",
    "    y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "    # define the model\n",
    "    pipe = Pipeline([('selector', VarianceThreshold(3)),\n",
    "                 ('classifier', SVC(kernel='linear'))])\n",
    "\n",
    "    search_space = [{'classifier': [SVC(kernel='linear')]}]\n",
    "    # define search\n",
    "    _info = GridSearchCV(pipe, search_space, scoring='accuracy', cv=None, refit=True)\n",
    "    # execute search\n",
    "    result = _info.fit(X_train, y_train)\n",
    "    \n",
    "    select_indexes = np.arange(len(X_train.columns)).reshape(1, -1)\n",
    "    select_indexes = result.best_estimator_.named_steps['selector'].transform(select_indexes)\n",
    "    selected_features = X.iloc[:,select_indexes.reshape(-1)]\n",
    "    x_t = X_test[selected_features.columns.values]\n",
    "    \n",
    "    clf = SVC(kernel='linear')\n",
    "    clf.fit(X_train[selected_features.columns.values], y_train)\n",
    "    # evaluate the model\n",
    "    acc = accuracy_score(y_test, clf.predict(x_t))\n",
    "    # store the result\n",
    "    outer_results.append(acc)\n",
    "    # report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_estimator_))\n",
    "# summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(outer_results), np.std(outer_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "pipe = Pipeline([('scaler', MinMaxScaler()),\n",
    "                 ('selector', SelectKBest(chi2, k='all')),\n",
    "                 ('classifier', SVC())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = [{'classifier': [SVC()]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 198 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('scaler', MinMaxScaler()),\n",
       "                                       ('selector',\n",
       "                                        SelectKBest(k='all',\n",
       "                                                    score_func=<function chi2 at 0x0000022562F90040>)),\n",
       "                                       ('classifier', SVC())]),\n",
       "             param_grid=[{'classifier': [SVC()]}])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "svm_chi = GridSearchCV(pipe, search_space, cv=None, verbose=0)\n",
    "svm_chi.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9912461652385487\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(svm_chi, X, y, cv=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">acc=0.993, est=0.979, cfg=Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('selector',\n",
      "                 SelectKBest(score_func=<function chi2 at 0x000001AA93E00DC0>)),\n",
      "                ('classifier', SVC(kernel='linear'))])\n",
      ">acc=0.985, est=0.975, cfg=Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('selector',\n",
      "                 SelectKBest(score_func=<function chi2 at 0x000001AA93E00DC0>)),\n",
      "                ('classifier', SVC(kernel='linear'))])\n",
      ">acc=0.989, est=0.978, cfg=Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('selector',\n",
      "                 SelectKBest(score_func=<function chi2 at 0x000001AA93E00DC0>)),\n",
      "                ('classifier', SVC(kernel='linear'))])\n",
      ">acc=0.985, est=0.980, cfg=Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('selector',\n",
      "                 SelectKBest(score_func=<function chi2 at 0x000001AA93E00DC0>)),\n",
      "                ('classifier', SVC(kernel='linear'))])\n",
      ">acc=0.993, est=0.982, cfg=Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('selector',\n",
      "                 SelectKBest(score_func=<function chi2 at 0x000001AA93E00DC0>)),\n",
      "                ('classifier', SVC(kernel='linear'))])\n",
      "Accuracy: 0.989 (0.003)\n",
      "Wall time: 708 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features = list(X.columns.values)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# enumerate splits\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in kf.split(X):\n",
    "    # split data\n",
    "    X_train, X_test = X.iloc[train_ix][features], X.iloc[test_ix][features]\n",
    "    y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "    # define the model\n",
    "    pipe = Pipeline([('scaler', MinMaxScaler()),\n",
    "                 ('selector', SelectKBest(chi2, k=10)),\n",
    "                 ('classifier', SVC(kernel='linear'))])\n",
    "    # define search space\n",
    "    search_space = [{'classifier': [SVC(kernel='linear')]}]\n",
    "    # define search\n",
    "    _info = GridSearchCV(pipe, search_space, scoring='accuracy', cv=None, refit=True)\n",
    "    # execute search\n",
    "    result = _info.fit(X_train, y_train)\n",
    "    \n",
    "    selected = result.best_estimator_.named_steps['selector'].get_support(indices=True)\n",
    "    selected_features = X.iloc[:,selected]\n",
    "    x_t = X_test[selected_features.columns.values]\n",
    "    \n",
    "    clf = SVC(kernel='linear')\n",
    "    clf.fit(X_train[selected_features.columns.values], y_train)\n",
    "    # evaluate the model\n",
    "    acc = accuracy_score(y_test, clf.predict(x_t))\n",
    "    # store the result\n",
    "    outer_results.append(acc)\n",
    "    # report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_estimator_))\n",
    "# summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(outer_results), np.std(outer_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "pipe = Pipeline([\n",
    "                 ('selector', SelectKBest(mutual_info_classif, k='all')),\n",
    "                 ('classifier', GaussianNB())])\n",
    "\n",
    "search_space = [{'selector__k': ['all']},\n",
    "                {'classifier': [GaussianNB()]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.32 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('selector',\n",
       "                                        SelectKBest(k='all',\n",
       "                                                    score_func=<function mutual_info_classif at 0x000002256416BB80>)),\n",
       "                                       ('classifier', GaussianNB())]),\n",
       "             param_grid=[{'selector__k': ['all']},\n",
       "                         {'classifier': [GaussianNB()]}])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "nb_info = GridSearchCV(pipe, search_space, cv=None, verbose=0)\n",
    "nb_info.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8498201629112451\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(nb_info, X, y, cv=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">acc=0.836, est=0.846, cfg={'selector__k': 10}\n",
      ">acc=0.858, est=0.834, cfg={'selector__k': 10}\n",
      ">acc=0.843, est=0.842, cfg={'selector__k': 10}\n",
      ">acc=0.814, est=0.840, cfg={'selector__k': 10}\n",
      ">acc=0.850, est=0.843, cfg={'selector__k': 10}\n",
      "Accuracy: 0.840 (0.015)\n",
      "Wall time: 6.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# enumerate splits\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in kf.split(X):\n",
    "    # split data\n",
    "    X_train, X_test = X.iloc[train_ix].loc[:, features], X.iloc[test_ix][features]\n",
    "    y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "    # define the model\n",
    "    pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('selector', SelectKBest(mutual_info_classif, k=10)),\n",
    "                 ('classifier', GaussianNB())])\n",
    "    # define search space\n",
    "    search_space = [{'selector__k': [10]}, {'classifier': [GaussianNB()]}]\n",
    "    # define search\n",
    "    _info = GridSearchCV(pipe, search_space, scoring='accuracy', cv=None, refit=True)\n",
    "    # execute search\n",
    "    result = _info.fit(X_train, y_train)\n",
    "    \n",
    "    selected = result.best_estimator_.named_steps['selector'].get_support(indices=True)\n",
    "    selected_features = X.iloc[:,selected]\n",
    "    x_t = X_test[selected_features.columns.values]\n",
    "    \n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_train[selected_features.columns.values], y_train)\n",
    "    # evaluate the model\n",
    "    acc = accuracy_score(y_test, clf.predict(x_t))\n",
    "    # store the result\n",
    "    outer_results.append(acc)\n",
    "    # report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_params_))\n",
    "# summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(outer_results), np.std(outer_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "pipe = Pipeline([('selector', VarianceThreshold()),\n",
    "                 ('classifier', GaussianNB())])\n",
    "\n",
    "search_space = [{'classifier': [GaussianNB()]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('selector', VarianceThreshold()),\n",
       "                                       ('classifier', GaussianNB())]),\n",
       "             param_grid=[{'classifier': [GaussianNB()]}], scoring='accuracy')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_variance = GridSearchCV(pipe, search_space, cv=None, verbose=0, scoring= 'accuracy')\n",
    "nb_variance.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8498201629112451\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(nb_variance, X, y, cv=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">acc=0.836, est=0.846, cfg=Pipeline(steps=[('selector', VarianceThreshold()),\n",
      "                ('classifier', GaussianNB())])\n",
      ">acc=0.858, est=0.834, cfg=Pipeline(steps=[('selector', VarianceThreshold()),\n",
      "                ('classifier', GaussianNB())])\n",
      ">acc=0.843, est=0.842, cfg=Pipeline(steps=[('selector', VarianceThreshold()),\n",
      "                ('classifier', GaussianNB())])\n",
      ">acc=0.814, est=0.840, cfg=Pipeline(steps=[('selector', VarianceThreshold()),\n",
      "                ('classifier', GaussianNB())])\n",
      ">acc=0.850, est=0.843, cfg=Pipeline(steps=[('selector', VarianceThreshold()),\n",
      "                ('classifier', GaussianNB())])\n",
      "Accuracy: 0.840 (0.015)\n",
      "Wall time: 438 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features = list(X.columns.values)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# enumerate splits\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in kf.split(X):\n",
    "    # split data\n",
    "    X_train, X_test = X.iloc[train_ix].loc[:, features], X.iloc[test_ix][features]\n",
    "    y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "    # define the model\n",
    "    pipe = Pipeline([('selector', VarianceThreshold()),\n",
    "                 ('classifier', GaussianNB())])\n",
    "\n",
    "    search_space = [{'classifier': [GaussianNB()]}]\n",
    "    # define search\n",
    "    _info = GridSearchCV(pipe, search_space, scoring='accuracy', cv=None, refit=True)\n",
    "    # execute search\n",
    "    result = _info.fit(X_train, y_train)\n",
    "    \n",
    "    select_indexes = np.arange(len(X_train.columns)).reshape(1, -1)\n",
    "    select_indexes = result.best_estimator_.named_steps['selector'].transform(select_indexes)\n",
    "    selected_features = X.iloc[:,select_indexes.reshape(-1)]\n",
    "    x_t = X_test[selected_features.columns.values]\n",
    "    \n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_train[selected_features.columns.values], y_train)\n",
    "    # evaluate the model\n",
    "    acc = accuracy_score(y_test, clf.predict(x_t))\n",
    "    # store the result\n",
    "    outer_results.append(acc)\n",
    "    # report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_estimator_))\n",
    "# summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(outer_results), np.std(outer_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "pipe = Pipeline([('scaler', MinMaxScaler()),\n",
    "                 ('selector', SelectKBest(chi2, k='all')),\n",
    "                 ('classifier', GaussianNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = [{'classifier': [GaussianNB()]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 113 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('scaler', MinMaxScaler()),\n",
       "                                       ('selector',\n",
       "                                        SelectKBest(k='all',\n",
       "                                                    score_func=<function chi2 at 0x0000022562F90040>)),\n",
       "                                       ('classifier', GaussianNB())]),\n",
       "             param_grid=[{'classifier': [GaussianNB()]}])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "nb_chi = GridSearchCV(pipe, search_space, cv=None, verbose=0)\n",
    "nb_chi.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8498201629112451\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(nb_chi, X, y, cv=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">acc=0.836, est=0.846, cfg=Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('selector',\n",
      "                 SelectKBest(score_func=<function chi2 at 0x000001AA93E00DC0>)),\n",
      "                ('classifier', GaussianNB())])\n",
      ">acc=0.858, est=0.834, cfg=Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('selector',\n",
      "                 SelectKBest(score_func=<function chi2 at 0x000001AA93E00DC0>)),\n",
      "                ('classifier', GaussianNB())])\n",
      ">acc=0.843, est=0.842, cfg=Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('selector',\n",
      "                 SelectKBest(score_func=<function chi2 at 0x000001AA93E00DC0>)),\n",
      "                ('classifier', GaussianNB())])\n",
      ">acc=0.814, est=0.840, cfg=Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('selector',\n",
      "                 SelectKBest(score_func=<function chi2 at 0x000001AA93E00DC0>)),\n",
      "                ('classifier', GaussianNB())])\n",
      ">acc=0.850, est=0.843, cfg=Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('selector',\n",
      "                 SelectKBest(score_func=<function chi2 at 0x000001AA93E00DC0>)),\n",
      "                ('classifier', GaussianNB())])\n",
      "Accuracy: 0.840 (0.015)\n",
      "Wall time: 385 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features = list(X.columns.values)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# enumerate splits\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in kf.split(X):\n",
    "    # split data\n",
    "    X_train, X_test = X.iloc[train_ix][features], X.iloc[test_ix][features]\n",
    "    y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "    # define the model\n",
    "    pipe = Pipeline([('scaler', MinMaxScaler()),\n",
    "                 ('selector', SelectKBest(chi2, k=10)),\n",
    "                 ('classifier', GaussianNB())])\n",
    "    # define search space\n",
    "    search_space = [{'classifier': [GaussianNB()]}]\n",
    "    # define search\n",
    "    _info = GridSearchCV(pipe, search_space, scoring='accuracy', cv=None, refit=True)\n",
    "    # execute search\n",
    "    result = _info.fit(X_train, y_train)\n",
    "    \n",
    "    selected = result.best_estimator_.named_steps['selector'].get_support(indices=True)\n",
    "    selected_features = X.iloc[:,selected]\n",
    "    x_t = X_test[selected_features.columns.values]\n",
    "    \n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_train[selected_features.columns.values], y_train)\n",
    "    # evaluate the model\n",
    "    acc = accuracy_score(y_test, clf.predict(x_t))\n",
    "    # store the result\n",
    "    outer_results.append(acc)\n",
    "    # report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_estimator_))\n",
    "# summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(outer_results), np.std(outer_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0939233870967742\n",
      "0.09518054623655914\n"
     ]
    }
   ],
   "source": [
    "### Information gain\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "pipe = Pipeline([\n",
    "                 ('selector', SelectKBest(mutual_info_classif, k='all')),\n",
    "                 ('classifier', RandomForestRegressor())])\n",
    "\n",
    "search_space = [{'selector__k': ['all']},\n",
    "                {'classifier': [RandomForestRegressor()]}]\n",
    "\n",
    "nb_info = GridSearchCV(pipe, search_space, cv=None, verbose=0)\n",
    "nb_info.fit(X_train, y_train)\n",
    "\n",
    "print(cross_val_score(nb_info, X, y, cv=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;31m# Compute accuracy for each possible representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'multilabel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0m\u001b[0;32m     91\u001b[0m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# enumerate splits\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in kf.split(X):\n",
    "    # split data\n",
    "    X_train, X_test = X.iloc[train_ix][features], X.iloc[test_ix][features]\n",
    "    y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "    # define the model\n",
    "    pipe = Pipeline([\n",
    "                 ('selector', SelectKBest(mutual_info_classif, k=10)),\n",
    "                 ('classifier', RandomForestRegressor())])\n",
    "\n",
    "    search_space = [{'selector__k': [10]},\n",
    "                {'classifier': [RandomForestRegressor()]}]\n",
    "    # define search\n",
    "    _info = GridSearchCV(pipe, search_space, cv=None, verbose=0)\n",
    "    # execute search\n",
    "    result = _info.fit(X_train, y_train)\n",
    "    \n",
    "    selected = result.best_estimator_.named_steps['selector'].get_support(indices=True)\n",
    "    selected_features = X.iloc[:,selected]\n",
    "    x_t = X_test[selected_features.columns.values]\n",
    "    \n",
    "    clf = RandomForestRegressor()\n",
    "    clf.fit(X_train[selected_features.columns.values], y_train)\n",
    "    # evaluate the model\n",
    "    acc = accuracy_score(y_test, clf.predict(x_t))\n",
    "    # store the result\n",
    "    outer_results.append(acc)\n",
    "    # report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_params_))\n",
    "# summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(outer_results), np.std(outer_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Chi-Square\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "pipe = Pipeline([('scaler', MinMaxScaler()),\n",
    "                 ('selector', SelectKBest(chi2, k='all')),\n",
    "                 ('classifier', RandomForestRegressor())])\n",
    "\n",
    "search_space = [{'classifier': [RandomForestRegressor()]}]\n",
    "\n",
    "nb_chi = GridSearchCV(pipe, search_space, cv=None, verbose=0)\n",
    "nb_chi.fit(X_train, y_train)\n",
    "\n",
    "print(cross_val_score(nb_chi, X, y, cv=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "features = list(X.columns.values)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# enumerate splits\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in kf.split(X):\n",
    "    # split data\n",
    "    X_train, X_test = X.iloc[train_ix][features], X.iloc[test_ix][features]\n",
    "    y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "    # define the model\n",
    "    pipe = Pipeline([('scaler', MinMaxScaler()),\n",
    "                 ('selector', SelectKBest(chi2, k='all')),\n",
    "                 ('classifier', RandomForestRegressor())])\n",
    "    # define search space\n",
    "    search_space = [{'classifier': [RandomForestRegressor()]}]\n",
    "    # define search\n",
    "    _info = GridSearchCV(pipe, search_space, cv=None, verbose=0)\n",
    "    # execute search\n",
    "    result = _info.fit(X_train, y_train)\n",
    "    selected = result.best_estimator_.named_steps['selector'].get_support(indices=True)\n",
    "    selected_features = X.iloc[:,selected]\n",
    "    x_t = X_test[selected_features.columns.values]\n",
    "    \n",
    "    clf = RandomForestRegressor()\n",
    "    clf.fit(X_train[selected_features.columns.values], y_train)\n",
    "    # evaluate the model\n",
    "    acc = accuracy_score(y_test, clf.predict(x_t))\n",
    "    # store the result\n",
    "    outer_results.append(acc)\n",
    "    # report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_estimator_))\n",
    "# summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(outer_results), np.std(outer_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9919760922458478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.994895800275045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Walter Ortiz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "pipe = Pipeline([\n",
    "                 ('selector', SelectKBest(mutual_info_classif, k='all')),\n",
    "                 ('classifier', MLPClassifier())])\n",
    "\n",
    "search_space = [{'selector__k': ['all']},\n",
    "                {'classifier': [MLPClassifier()]}]\n",
    "\n",
    "nb_info = GridSearchCV(pipe, search_space, cv=None, verbose=0)\n",
    "nb_info.fit(X_train, y_train)\n",
    "\n",
    "print(cross_val_score(nb_info, X, y, cv=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">acc=1.000, est=0.998, cfg={'selector__k': 10}\n",
      ">acc=1.000, est=0.999, cfg={'selector__k': 10}\n",
      ">acc=1.000, est=1.000, cfg={'selector__k': 10}\n",
      ">acc=1.000, est=0.999, cfg={'selector__k': 10}\n",
      ">acc=1.000, est=0.999, cfg={'selector__k': 10}\n",
      "Accuracy: 1.000 (0.000)\n",
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# enumerate splits\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in kf.split(X):\n",
    "    # split data\n",
    "    X_train, X_test = X.iloc[train_ix].loc[:, features], X.iloc[test_ix][features]\n",
    "    y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "    # define the model\n",
    "    pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('selector', SelectKBest(mutual_info_classif, k=10)),\n",
    "                 ('classifier', MLPClassifier())])\n",
    "    # define search space\n",
    "    search_space = [{'selector__k': [10]}, {'classifier': [MLPClassifier()]}]\n",
    "    # define search\n",
    "    _info = GridSearchCV(pipe, search_space, scoring='accuracy', cv=None, refit=True)\n",
    "    # execute search\n",
    "    result = _info.fit(X_train, y_train)\n",
    "    \n",
    "    selected = result.best_estimator_.named_steps['selector'].get_support(indices=True)\n",
    "    selected_features = X.iloc[:,selected]\n",
    "    x_t = X_test[selected_features.columns.values]\n",
    "    \n",
    "    clf = MLPClassifier()\n",
    "    clf.fit(X_train[selected_features.columns.values], y_train)\n",
    "    # evaluate the model\n",
    "    acc = accuracy_score(y_test, clf.predict(x_t))\n",
    "    # store the result\n",
    "    outer_results.append(acc)\n",
    "    # report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_params_))\n",
    "# summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(outer_results), np.std(outer_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Variance threshold\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "pipe = Pipeline([('scaler', MinMaxScaler()),\n",
    "                 ('selector', VarianceThreshold()),\n",
    "                 ('classifier', MLPClassifier())])\n",
    "\n",
    "search_space = [{'classifier': [MLPClassifier()]}]\n",
    "\n",
    "nb_variance = GridSearchCV(pipe, search_space, cv=None, verbose=0, scoring= 'accuracy')\n",
    "nb_variance.fit(X_train, y_train)\n",
    "\n",
    "print(cross_val_score(nb_variance, X, y, cv=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">acc=1.000, est=1.000, cfg=Pipeline(steps=[('selector', VarianceThreshold()),\n",
      "                ('classifier', MLPClassifier())])\n",
      ">acc=1.000, est=1.000, cfg=Pipeline(steps=[('selector', VarianceThreshold()),\n",
      "                ('classifier', MLPClassifier())])\n",
      ">acc=1.000, est=1.000, cfg=Pipeline(steps=[('selector', VarianceThreshold()),\n",
      "                ('classifier', MLPClassifier())])\n",
      ">acc=1.000, est=1.000, cfg=Pipeline(steps=[('selector', VarianceThreshold()),\n",
      "                ('classifier', MLPClassifier())])\n",
      ">acc=1.000, est=1.000, cfg=Pipeline(steps=[('selector', VarianceThreshold()),\n",
      "                ('classifier', MLPClassifier())])\n",
      "Accuracy: 1.000 (0.000)\n",
      "Wall time: 44.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features = list(X.columns.values)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# enumerate splits\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in kf.split(X):\n",
    "    # split data\n",
    "    X_train, X_test = X.iloc[train_ix].loc[:, features], X.iloc[test_ix][features]\n",
    "    y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "    # define the model\n",
    "    pipe = Pipeline([('selector', VarianceThreshold()),\n",
    "                 ('classifier', MLPClassifier())])\n",
    "\n",
    "    search_space = [{'classifier': [MLPClassifier()]}]\n",
    "    # define search\n",
    "    _info = GridSearchCV(pipe, search_space, scoring='accuracy', cv=None, refit=True)\n",
    "    # execute search\n",
    "    result = _info.fit(X_train, y_train)\n",
    "    \n",
    "    select_indexes = np.arange(len(X_train.columns)).reshape(1, -1)\n",
    "    select_indexes = result.best_estimator_.named_steps['selector'].transform(select_indexes)\n",
    "    selected_features = X.iloc[:,select_indexes.reshape(-1)]\n",
    "    x_t = X_test[selected_features.columns.values]\n",
    "    \n",
    "    clf = MLPClassifier()\n",
    "    clf.fit(X_train[selected_features.columns.values], y_train)\n",
    "    # evaluate the model\n",
    "    acc = accuracy_score(y_test, clf.predict(x_t))\n",
    "    # store the result\n",
    "    outer_results.append(acc)\n",
    "    # report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_estimator_))\n",
    "# summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(outer_results), np.std(outer_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Chi-Square\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "pipe = Pipeline([('scaler', MinMaxScaler()),\n",
    "                 ('selector', SelectKBest(chi2, k='all')),\n",
    "                 ('classifier', MLPClassifier())])\n",
    "\n",
    "search_space = [{'classifier': [MLPClassifier()]}]\n",
    "\n",
    "nb_chi = GridSearchCV(pipe, search_space, cv=None, verbose=0)\n",
    "nb_chi.fit(X_train, y_train)\n",
    "\n",
    "print(cross_val_score(nb_chi, X, y, cv=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">acc=1.000, est=0.989, cfg=Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('selector',\n",
      "                 SelectKBest(score_func=<function chi2 at 0x000001AA93E00DC0>)),\n",
      "                ('classifier', MLPClassifier())])\n",
      ">acc=1.000, est=0.989, cfg=Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('selector',\n",
      "                 SelectKBest(score_func=<function chi2 at 0x000001AA93E00DC0>)),\n",
      "                ('classifier', MLPClassifier())])\n",
      ">acc=1.000, est=0.987, cfg=Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('selector',\n",
      "                 SelectKBest(score_func=<function chi2 at 0x000001AA93E00DC0>)),\n",
      "                ('classifier', MLPClassifier())])\n",
      ">acc=1.000, est=0.985, cfg=Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('selector',\n",
      "                 SelectKBest(score_func=<function chi2 at 0x000001AA93E00DC0>)),\n",
      "                ('classifier', MLPClassifier())])\n",
      ">acc=1.000, est=0.985, cfg=Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('selector',\n",
      "                 SelectKBest(score_func=<function chi2 at 0x000001AA93E00DC0>)),\n",
      "                ('classifier', MLPClassifier())])\n",
      "Accuracy: 1.000 (0.000)\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features = list(X.columns.values)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# enumerate splits\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in kf.split(X):\n",
    "    # split data\n",
    "    X_train, X_test = X.iloc[train_ix][features], X.iloc[test_ix][features]\n",
    "    y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "    # define the model\n",
    "    pipe = Pipeline([('scaler', MinMaxScaler()),\n",
    "                 ('selector', SelectKBest(chi2, k=10)),\n",
    "                 ('classifier', MLPClassifier())])\n",
    "    # define search space\n",
    "    search_space = [{'classifier': [MLPClassifier()]}]\n",
    "    # define search\n",
    "    _info = GridSearchCV(pipe, search_space, scoring='accuracy', cv=None, refit=True)\n",
    "    # execute search\n",
    "    result = _info.fit(X_train, y_train)\n",
    "    \n",
    "    selected = result.best_estimator_.named_steps['selector'].get_support(indices=True)\n",
    "    selected_features = X.iloc[:,selected]\n",
    "    x_t = X_test[selected_features.columns.values]\n",
    "    \n",
    "    clf = MLPClassifier()\n",
    "    clf.fit(X_train[selected_features.columns.values], y_train)\n",
    "    # evaluate the model\n",
    "    acc = accuracy_score(y_test, clf.predict(x_t))\n",
    "    # store the result\n",
    "    outer_results.append(acc)\n",
    "    # report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_estimator_))\n",
    "# summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(outer_results), np.std(outer_results)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
